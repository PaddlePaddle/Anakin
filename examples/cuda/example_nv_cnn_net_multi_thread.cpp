
#include "utils/logger/logger.h"
#include "graph.h"
#include "net.h"

/*worker is anakin thread pool*/
#include "worker.h"

/*util to fill tensor*/
#include "saber/core/tensor_op.h"
#ifdef USE_CUDA
using namespace anakin;
using namespace anakin::graph;
using namespace anakin::saber;

int main(int argc, const char** argv) {

    /*init works object by model path and thread pool size*/
    Worker<NV, AK_FLOAT, Precision::FP32>  workers("Resnet50.anakin.bin", 10);
    workers.register_inputs({"input_0"});
    workers.register_outputs({"prob_out"});
    /*set input shape*/
    workers.Reshape("input_0", {1, 3, 224, 224});
    /*start workers*/
    workers.launch();

    /*fill input*/
    std::vector<Tensor4dPtr<target_host<NV>::type, AK_FLOAT> > host_tensor_p_in_list;
    saber::Shape valid_shape_in({1, 3, 224, 224});
    Tensor4dPtr<target_host<NV>::type, AK_FLOAT> h_tensor_in = new Tensor4d<target_host<NV>::type, AK_FLOAT>(valid_shape_in);
    float* h_data = h_tensor_in->mutable_data();
    for (int i=0; i<h_tensor_in->size(); i++) {
        h_data[i] = 1.0f;
    }
    host_tensor_p_in_list.push_back(h_tensor_in);


    /*run inferï¼Œsend input to worker queue*/
    int epoch = 1000;
    for(int i=0; i<epoch; i++) {
        auto  d_tensor_p_out_list = workers.sync_prediction(host_tensor_p_in_list);
        auto d_tensor_p = d_tensor_p_out_list[0];
    }
    LOG(INFO)<<"info finish";

}
#else
#include "stdio.h"
int main(){printf("nothing happened -_-!!\n");}
#endif